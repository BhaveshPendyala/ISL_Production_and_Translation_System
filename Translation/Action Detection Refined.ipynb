{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['I', 'go', 'cinema', 'yesterday','return', 'friend', 'death','son', 'hate', 'father'])\n",
    "# actions = np.array(['son', 'hate', 'father'])\n",
    "\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MADHAV\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define model-building function\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(\n",
    "    units=128,\n",
    "    return_sequences=True, input_shape=(30, 1662)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(\n",
    "    units=128))\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dense(len(actions), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">688,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">970</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m688,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m12,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m970\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">800,554</span> (3.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m800,554\u001b[0m (3.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">800,554</span> (3.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m800,554\u001b[0m (3.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('10gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), ()]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MADHAV\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Configure Google Gemini API\n",
    "genai.configure(api_key=\"AIzaSyC0Lk1hGsHnFzT6QE6yACy7Uc9BIU4cTSw\")\n",
    "model_gemini = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "\n",
    "# Load conversation dynamically\n",
    "conversation_file = \"messages.txt\"\n",
    "conversation_update_interval = 2\n",
    "last_update_time = 0\n",
    "\n",
    "def load_conversation(file_path):\n",
    "    extracted_sentences = []\n",
    "    pattern = r'\\d+\\. normal_user:\\s*(.*)'\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                match = re.match(pattern, line.strip())\n",
    "                if match:\n",
    "                    extracted_sentences.append(match.group(1))\n",
    "        return extracted_sentences\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "    \n",
    "# Save generated LLM sentences with indexing\n",
    "def save_llm_response(response, filename=\"messages.txt\"):\n",
    "    current_index = 1\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            current_index = len(file.readlines()) + 1\n",
    "\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{current_index}. sign_user:{response}\\n\")\n",
    "\n",
    "# Generate meaningful sentences using Gemini AI\n",
    "def generate_sentence_gemini(input_tokens, previous_sentence):\n",
    "    prompt = f\"\"\"\n",
    "    This is a real-time conversation about a murder case.\n",
    "    - Normal person said: \"{previous_sentence}\"\n",
    "    - Sign language tokens: {' '.join(input_tokens)}\n",
    "    \n",
    "    Convert these tokens into a grammatically correct response as if the sign user is answering.\n",
    "    Give it as a sentence (dont give explanations)\n",
    "    \"\"\"\n",
    "    response = model_gemini.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "# Save conversation log\n",
    "def save_conversation_log(logs, filename=\"conversation_log.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(\"\\n\".join(logs))\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "saved_confidence_scores = []\n",
    "threshold = 0.7\n",
    "conversation_index = 0\n",
    "conversation_log = []\n",
    "\n",
    "conversation_file = \"messages.txt\"\n",
    "conversation = load_conversation(conversation_file)\n",
    "last_update_time = time.time()\n",
    "update_interval = 2\n",
    "\n",
    "# with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         if time.time() - last_update_time >= conversation_update_interval:\n",
    "#             conversation = load_conversation(\"messages.txt\")\n",
    "#             last_update_time = time.time()\n",
    "\n",
    "#         if conversation_index < len(conversation):\n",
    "#             normal_sentence = conversation[conversation_index]\n",
    "#         else:\n",
    "#             normal_sentence = \"Waiting for new messages...\"\n",
    "\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "#         if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "#             keypoints = extract_keypoints(results)\n",
    "#             sequence.append(keypoints)\n",
    "#             sequence = sequence[-30:]\n",
    "\n",
    "#             if len(sequence) == 30:\n",
    "#                 res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#                 predicted_word = actions[np.argmax(res)]\n",
    "#                 predictions.append(np.argmax(res))\n",
    "#                 confidence_score = res[np.argmax(res)]\n",
    "\n",
    "#                 if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "#                     if confidence_score > threshold:\n",
    "#                         if len(sentence) == 0 or (predicted_word != sentence[-1]):\n",
    "#                             sentence.append(predicted_word)\n",
    "#                             saved_confidence_scores.append(confidence_score)\n",
    "#         else:\n",
    "                    \n",
    "#             sequence.clear()\n",
    "#             print(\"Hands out of frame: Resetting keypoints and predictions.\")\n",
    "\n",
    "#         cv2.rectangle(image, (0, 0), (640, 50), (245, 117, 16), -1)\n",
    "#         cv2.putText(image, f\"Normal: {normal_sentence}\", (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "#         cv2.putText(image, f\"Sign: {' '.join(sentence)}\", (3, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "         \n",
    "#         # Draw background for confidence scores\n",
    "#         cv2.rectangle(image, (0, 60), (80, 600), (50, 50, 50), -1)\n",
    "        \n",
    "#         # Display saved confidence scores below the text\n",
    "#         if saved_confidence_scores:\n",
    "#             for i, (word, score) in enumerate(zip(sentence, saved_confidence_scores)):\n",
    "#                 cv2.putText(image, f\"{word}: {score:.2f}\", (10, 80 + (i * 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "#         # Show the output\n",
    "       \n",
    "#         cv2.imshow('Real-Time Sign Language Conversation', image)\n",
    "\n",
    "#         key = cv2.waitKey(10) & 0xFF\n",
    "#         if key == ord('s'):\n",
    "#             if sentence:\n",
    "#                 meaningful_response = generate_sentence_gemini(sentence, normal_sentence)\n",
    "#                 conversation_log.append(f\"Normal: {normal_sentence}\")\n",
    "#                 conversation_log.append(f\"Sign: {meaningful_response}\")\n",
    "#                 save_llm_response(meaningful_response)\n",
    "\n",
    "#             sentence.clear()\n",
    "#             saved_confidence_scores.clear()\n",
    "#             conversation_index += 1\n",
    "\n",
    "#         elif key == ord('r') and sentence:\n",
    "#             sentence.pop()\n",
    "#             saved_confidence_scores.pop()\n",
    "\n",
    "#         elif key == ord('q'):\n",
    "#             break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# save_conversation_log(conversation_log)\n",
    "# print(\"Conversation log saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label, Button, Frame\n",
    "# from PIL import Image, ImageTk\n",
    "\n",
    "# # Initialize variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# saved_confidence_scores = []\n",
    "# threshold = 0.7\n",
    "# conversation_index = 0\n",
    "# conversation_log = []\n",
    "# normal_sentence = \"Waiting for new messages...\"\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initialize MediaPipe Holistic\n",
    "# detection_model = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# def update_ui():\n",
    "#     global sentence, normal_sentence\n",
    "#     sentence_label.config(text=f\"Sign: {' '.join(sentence)}\")\n",
    "#     normal_label.config(text=f\"Normal: {normal_sentence}\")\n",
    "    \n",
    "#     confidence_text = \"\\n\".join([f\"{word}: {score:.2f}\" for word, score in zip(sentence, saved_confidence_scores)])\n",
    "#     confidence_label.config(text=f\"Confidence:\\n{confidence_text}\")\n",
    "    \n",
    "#     root.after(100, update_ui)\n",
    "\n",
    "# def process_frame():\n",
    "#     global sequence, sentence, predictions, saved_confidence_scores, conversation_index, normal_sentence\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         return\n",
    "    \n",
    "#     image, results = mediapipe_detection(frame, detection_model)\n",
    "    \n",
    "#     if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-30:]\n",
    "\n",
    "#         if len(sequence) == 30:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#             predicted_word = actions[np.argmax(res)]\n",
    "#             predictions.append(np.argmax(res))\n",
    "#             confidence_score = res[np.argmax(res)]\n",
    "\n",
    "#             if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "#                 if confidence_score > threshold:\n",
    "#                     if len(sentence) == 0 or (predicted_word != sentence[-1]):\n",
    "#                         sentence.append(predicted_word)\n",
    "#                         saved_confidence_scores.append(confidence_score)\n",
    "#     else:\n",
    "#         sequence.clear()\n",
    "    \n",
    "#     # Convert frame to Tkinter format\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     img = Image.fromarray(frame)\n",
    "#     imgtk = ImageTk.PhotoImage(image=img)\n",
    "#     video_label.imgtk = imgtk\n",
    "#     video_label.configure(image=imgtk)\n",
    "    \n",
    "#     root.after(10, process_frame)\n",
    "\n",
    "# def save_sentence():\n",
    "#     global sentence, conversation_index\n",
    "#     if sentence:\n",
    "#         meaningful_response = generate_sentence_gemini(sentence, normal_sentence)\n",
    "#         conversation_log.append(f\"Normal: {normal_sentence}\")\n",
    "#         conversation_log.append(f\"Sign: {meaningful_response}\")\n",
    "#         save_llm_response(meaningful_response)\n",
    "#     sentence.clear()\n",
    "#     saved_confidence_scores.clear()\n",
    "#     conversation_index += 1\n",
    "\n",
    "# def remove_last_word():\n",
    "#     if sentence:\n",
    "#         sentence.pop()\n",
    "#         saved_confidence_scores.pop()\n",
    "\n",
    "# def quit_application():\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     save_conversation_log(conversation_log)\n",
    "#     root.destroy()\n",
    "\n",
    "# # UI Setup\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Sign Language Prediction UI\")\n",
    "# root.geometry(\"800x600\")\n",
    "# root.configure(bg=\"#2C3E50\")\n",
    "\n",
    "# frame = Frame(root, bg=\"#34495E\", padx=10, pady=10)\n",
    "# frame.pack(pady=10)\n",
    "\n",
    "# video_label = Label(frame, bg=\"#34495E\")\n",
    "# video_label.pack()\n",
    "\n",
    "# normal_label = Label(root, text=\"Normal: Waiting for new messages...\", font=(\"Arial\", 14), bg=\"#2C3E50\", fg=\"white\")\n",
    "# normal_label.pack(pady=5)\n",
    "\n",
    "# sentence_label = Label(root, text=\"Sign: \", font=(\"Arial\", 14), bg=\"#2C3E50\", fg=\"white\")\n",
    "# sentence_label.pack(pady=5)\n",
    "\n",
    "# confidence_label = Label(root, text=\"Confidence:\", font=(\"Arial\", 14), bg=\"#2C3E50\", fg=\"white\")\n",
    "# confidence_label.pack(pady=5)\n",
    "\n",
    "# button_frame = Frame(root, bg=\"#2C3E50\")\n",
    "# button_frame.pack(pady=10)\n",
    "\n",
    "# save_button = Button(button_frame, text=\"Save (S)\", command=save_sentence, font=(\"Arial\", 12), bg=\"#27AE60\", fg=\"white\", padx=10, pady=5)\n",
    "# save_button.grid(row=0, column=0, padx=10)\n",
    "\n",
    "# remove_button = Button(button_frame, text=\"Remove Last (R)\", command=remove_last_word, font=(\"Arial\", 12), bg=\"#E67E22\", fg=\"white\", padx=10, pady=5)\n",
    "# remove_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "# quit_button = Button(button_frame, text=\"Quit (Q)\", command=quit_application, font=(\"Arial\", 12), bg=\"#C0392B\", fg=\"white\", padx=10, pady=5)\n",
    "# quit_button.grid(row=0, column=2, padx=10)\n",
    "\n",
    "# # Start UI Update and Video Processing\n",
    "# update_ui()\n",
    "# process_frame()\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import websockets\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import base64\n",
    "# import io\n",
    "# import os\n",
    "# import cv2\n",
    "# import nest_asyncio\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label, Button, Frame\n",
    "# from PIL import Image, ImageTk\n",
    "# from threading import Thread\n",
    "# import mediapipe as mp\n",
    "# import keyboard\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Initialize variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# saved_confidence_scores = []\n",
    "# threshold = 0.7\n",
    "# conversation_log = []\n",
    "# normal_sentence = \"Waiting for new messages...\"\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# received_video_path = \"final_skeleton_output.mp4\"\n",
    "# MESSAGE_FILE = \"messages.txt\"\n",
    "\n",
    "# # Initialize MediaPipe Holistic\n",
    "# detection_model = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# def update_ui():\n",
    "#     sentence_label.config(text=f\"Sign: {' '.join(sentence)}\")\n",
    "#     normal_label.config(text=f\"Normal: {normal_sentence}\")\n",
    "#     confidence_text = \"\\n\".join([f\"{word}: {score:.2f}\" for word, score in zip(sentence, saved_confidence_scores)])\n",
    "#     confidence_label.config(text=f\"Confidence:\\n{confidence_text}\")\n",
    "#     root.after(100, update_ui)\n",
    "\n",
    "# def process_frame():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         return\n",
    "    \n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = detection_model.process(frame)\n",
    "    \n",
    "#     if results.pose_landmarks:\n",
    "#         keypoints = np.array([[lmk.x, lmk.y, lmk.z] for lmk in results.pose_landmarks.landmark]).flatten()\n",
    "#         sequence.append(keypoints)\n",
    "#         if len(sequence) > 30:\n",
    "#             sequence.pop(0)\n",
    "        \n",
    "#     img = Image.fromarray(frame)\n",
    "#     imgtk = ImageTk.PhotoImage(image=img)\n",
    "#     video_label.imgtk = imgtk\n",
    "#     video_label.configure(image=imgtk)\n",
    "#     root.after(10, process_frame)\n",
    "\n",
    "# def save_sentence():\n",
    "#     global sentence\n",
    "#     if sentence:\n",
    "#         conversation_log.append(f\"Sign: {' '.join(sentence)}\")\n",
    "#     sentence.clear()\n",
    "#     saved_confidence_scores.clear()\n",
    "\n",
    "# def remove_last_word():\n",
    "#     if sentence:\n",
    "#         sentence.pop()\n",
    "#         saved_confidence_scores.pop()\n",
    "\n",
    "# def quit_application():\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     root.destroy()\n",
    "\n",
    "# def visualize_skeleton(npy_array, output_path):\n",
    "#     frame_h, frame_w, fps = 720, 1280, 40\n",
    "#     video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_w, frame_h))\n",
    "#     for keypoints in npy_array:\n",
    "#         frame = np.zeros((frame_h, frame_w, 3), dtype=np.uint8)\n",
    "#         keypoints = keypoints.reshape(-1, 3)\n",
    "#         for x, y, _ in keypoints:\n",
    "#             if x > 0 and y > 0:\n",
    "#                 cv2.circle(frame, (int(x * frame_w), int(y * frame_h)), 4, (50, 205, 50), -1)\n",
    "#         video_out.write(frame)\n",
    "#     video_out.release()\n",
    "\n",
    "# def update_received_video():\n",
    "#     cap_received = cv2.VideoCapture(received_video_path)\n",
    "#     def show_video():\n",
    "#         while cap_received.isOpened():\n",
    "#             ret, frame = cap_received.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             img = Image.fromarray(frame)\n",
    "#             imgtk = ImageTk.PhotoImage(image=img)\n",
    "#             received_video_label.imgtk = imgtk\n",
    "#             received_video_label.configure(image=imgtk)\n",
    "#             root.update_idletasks()\n",
    "#         cap_received.release()\n",
    "#     Thread(target=show_video, daemon=True).start()\n",
    "\n",
    "# async def translation_server(websocket):\n",
    "#     try:\n",
    "#         async for received_data in websocket:\n",
    "#             data = json.loads(received_data)\n",
    "#             question = data.get(\"question\", \"\")\n",
    "#             npy_base64 = data.get(\"npy\", None)\n",
    "            \n",
    "#             if npy_base64:\n",
    "#                 npy_bytes = base64.b64decode(npy_base64)\n",
    "#                 npy_array = np.load(io.BytesIO(npy_bytes))\n",
    "#                 np.save(\"received.npy\", npy_array)\n",
    "#                 visualize_skeleton(npy_array, received_video_path)\n",
    "#                 update_received_video()\n",
    "            \n",
    "#             with open(MESSAGE_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(f\"normal_user: {question}\\n\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error: {e}\")\n",
    "\n",
    "# async def start_server():\n",
    "#     async with websockets.serve(translation_server, \"192.168.84.139\", 8767):\n",
    "#         print(\"✅ WebSocket Server Running...\")\n",
    "#         await asyncio.Future()\n",
    "\n",
    "# def send_to_client():\n",
    "#     message = sentence_label.cget(\"text\").replace(\"Sign: \", \"\").strip()\n",
    "#     asyncio.run(send_message(message))\n",
    "\n",
    "# server_thread = Thread(target=lambda: asyncio.run(start_server()), daemon=True)\n",
    "# server_thread.start()\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Sign Language UI with WebSocket\")\n",
    "# root.geometry(\"800x700\")\n",
    "# root.configure(bg=\"#2C3E50\")\n",
    "\n",
    "# video_label = Label(root, bg=\"#34495E\")\n",
    "# video_label.pack()\n",
    "\n",
    "# sentence_label = Label(root, text=\"Sign: \", font=(\"Arial\", 14), bg=\"#2C3E50\", fg=\"white\")\n",
    "# sentence_label.pack(pady=5)\n",
    "\n",
    "# confidence_label = Label(root, text=\"Confidence:\", font=(\"Arial\", 14), bg=\"#2C3E50\", fg=\"white\")\n",
    "# confidence_label.pack(pady=5)\n",
    "\n",
    "# button_frame = Frame(root, bg=\"#2C3E50\")\n",
    "# button_frame.pack(pady=10)\n",
    "\n",
    "# save_button = Button(button_frame, text=\"Save\", command=save_sentence, font=(\"Arial\", 12), bg=\"#27AE60\", fg=\"white\", padx=10, pady=5)\n",
    "# save_button.grid(row=0, column=0, padx=10)\n",
    "\n",
    "# remove_button = Button(button_frame, text=\"Remove Last\", command=remove_last_word, font=(\"Arial\", 12), bg=\"#E67E22\", fg=\"white\", padx=10, pady=5)\n",
    "# remove_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "# send_button = Button(button_frame, text=\"Send\", command=send_to_client, font=(\"Arial\", 12), bg=\"#3498DB\", fg=\"white\", padx=10, pady=5)\n",
    "# send_button.grid(row=0, column=2, padx=10)\n",
    "\n",
    "# received_video_label = Label(root, bg=\"#34495E\")\n",
    "# received_video_label.pack()\n",
    "\n",
    "# process_frame()\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label, Button, Frame\n",
    "# from PIL import Image, ImageTk\n",
    "# import asyncio\n",
    "# import websockets\n",
    "# import json\n",
    "# import base64\n",
    "# import io\n",
    "# import os\n",
    "# import keyboard\n",
    "# import nest_asyncio\n",
    "# import re\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Initialize variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# saved_confidence_scores = []\n",
    "# threshold = 0.7\n",
    "# conversation_index = 0\n",
    "# conversation_log = []\n",
    "# normal_sentence = \"Waiting for new messages...\"\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initialize MediaPipe Holistic\n",
    "# detection_model = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# MESSAGE_FILE = \"messages.txt\"\n",
    "\n",
    "# def get_next_index():\n",
    "#     if not os.path.exists(MESSAGE_FILE):\n",
    "#         return 1\n",
    "#     with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#         indices = [int(line.split('.')[0]) for line in lines if line.strip() and '.' in line]\n",
    "#         return max(indices) + 1 if indices else 1\n",
    "\n",
    "# def get_last_sign_user_message():\n",
    "#     try:\n",
    "#         with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in reversed(lines):\n",
    "#                 if \"sign_user:\" in line:\n",
    "#                     clean_message = line.replace(\"sign_user:\", \"\").strip()\n",
    "#                     clean_message = re.sub(r'\\d+', '', clean_message)\n",
    "#                     return clean_message.strip()\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"messages.txt not found.\")\n",
    "#     return None\n",
    "\n",
    "# def update_ui():\n",
    "#     global sentence, normal_sentence\n",
    "#     sentence_label.config(text=f\"Sign: {' '.join(sentence)}\")\n",
    "#     normal_label.config(text=f\"Normal: {normal_sentence}\")\n",
    "    \n",
    "#     confidence_text = \"\\n\".join([f\"{word}: {score:.2f}\" for word, score in zip(sentence, saved_confidence_scores)])\n",
    "#     confidence_label.config(text=f\"Confidence:\\n{confidence_text}\")\n",
    "    \n",
    "#     root.after(100, update_ui)\n",
    "\n",
    "# def process_frame():\n",
    "#     global sequence, sentence, predictions, saved_confidence_scores, conversation_index, normal_sentence\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         return\n",
    "    \n",
    "#     image, results = mediapipe_detection(frame, detection_model)\n",
    "    \n",
    "#     if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-30:]\n",
    "\n",
    "#         if len(sequence) == 30:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#             predicted_word = actions[np.argmax(res)]\n",
    "#             predictions.append(np.argmax(res))\n",
    "#             confidence_score = res[np.argmax(res)]\n",
    "\n",
    "#             if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "#                 if confidence_score > threshold:\n",
    "#                     if len(sentence) == 0 or (predicted_word != sentence[-1]):\n",
    "#                         sentence.append(predicted_word)\n",
    "#                         saved_confidence_scores.append(confidence_score)\n",
    "#     else:\n",
    "#         sequence.clear()\n",
    "    \n",
    "#     # Convert frame to Tkinter format\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     img = Image.fromarray(frame)\n",
    "#     imgtk = ImageTk.PhotoImage(image=img)\n",
    "#     video_label.imgtk = imgtk\n",
    "#     video_label.configure(image=imgtk)\n",
    "    \n",
    "#     root.after(10, process_frame)\n",
    "\n",
    "\n",
    "# def save_sentence():\n",
    "#     global sentence, conversation_index\n",
    "#     if sentence:\n",
    "#         meaningful_response = generate_sentence_gemini(sentence, normal_sentence)\n",
    "#         conversation_log.append(f\"Normal: {normal_sentence}\")\n",
    "#         conversation_log.append(f\"Sign: {meaningful_response}\")\n",
    "#         save_llm_response(meaningful_response)\n",
    "#     sentence.clear()\n",
    "#     saved_confidence_scores.clear()\n",
    "#     conversation_index += 1\n",
    "\n",
    "\n",
    "# def remove_last_word():\n",
    "#     if sentence:\n",
    "#         sentence.pop()\n",
    "#         saved_confidence_scores.pop()\n",
    "\n",
    "# def quit_application():\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     root.quit()\n",
    "#     root.destroy()\n",
    "\n",
    "# async def check_c_key(websocket):\n",
    "#     while True:\n",
    "#         await asyncio.sleep(0.1)\n",
    "#         if keyboard.is_pressed(\"c\"):\n",
    "#             sign_user_message = get_last_sign_user_message()\n",
    "#             if sign_user_message:\n",
    "#                 response_to_production = {\"sign_user_message\": sign_user_message}\n",
    "#                 await websocket.send(json.dumps(response_to_production))\n",
    "#                 print(f\"✅ Sent sign_user message: {sign_user_message}\")\n",
    "#                 await asyncio.sleep(1)\n",
    "\n",
    "# async def translation_server(websocket):\n",
    "#     print(\"✅ Production connected.\")\n",
    "#     asyncio.create_task(check_c_key(websocket))\n",
    "\n",
    "#     try:\n",
    "#         async for received_data in websocket:\n",
    "#             print(f\"🛠 DEBUG: Received Raw Data: {received_data}\")\n",
    "#             data = json.loads(received_data)\n",
    "#             question = data.get(\"question\", \"\")\n",
    "\n",
    "#             print(f\"🟢 Received question from normal_user: {question}\")\n",
    "#             index = get_next_index()\n",
    "\n",
    "#             with open(MESSAGE_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(f\"{index}. normal_user: {question}\\n\")\n",
    "\n",
    "#             answer = f\"Processed message #{index}\"\n",
    "#             response = {\"answer\": answer}\n",
    "#             await websocket.send(json.dumps(response))\n",
    "#             print(f\"✅ Sent response to Production: {answer}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Unexpected error: {e}\")\n",
    "\n",
    "# def send_to_client():\n",
    "#     message = sentence_label.cget(\"text\").replace(\"Sign: \", \"\").strip()\n",
    "#     asyncio.run(send_message(message))\n",
    "\n",
    "# async def main():\n",
    "#     async with websockets.serve(translation_server, \"192.168.84.139\", 8769, max_size=50 * 1024 * 1024, ping_interval=None):\n",
    "#         print(\"✅ Translation Server running at ws://192.168.84.139:8769 (Max size: 50MB)\")\n",
    "#         await asyncio.Future()\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Sign Language Prediction UI\")\n",
    "# root.geometry(\"900x700\")\n",
    "# root.configure(bg=\"#1A1A2E\")\n",
    "\n",
    "# frame = Frame(root, bg=\"#16213E\", padx=10, pady=10)\n",
    "# frame.pack(pady=10)\n",
    "\n",
    "# video_label = Label(frame, bg=\"#16213E\")\n",
    "# video_label.pack()\n",
    "\n",
    "# normal_label = Label(root, text=\"Normal: Waiting for new messages...\", font=(\"Arial\", 16), bg=\"#1A1A2E\", fg=\"white\")\n",
    "# normal_label.pack(pady=5)\n",
    "\n",
    "# sentence_label = Label(root, text=\"Sign: \", font=(\"Arial\", 16), bg=\"#1A1A2E\", fg=\"white\")\n",
    "# sentence_label.pack(pady=5)\n",
    "\n",
    "# confidence_label = Label(root, text=\"Confidence:\", font=(\"Arial\", 16), bg=\"#1A1A2E\", fg=\"white\")\n",
    "# confidence_label.pack(pady=5)\n",
    "\n",
    "# button_frame = Frame(root, bg=\"#1A1A2E\")\n",
    "# button_frame.pack(pady=10)\n",
    "\n",
    "# save_button = Button(button_frame, text=\"Save (S)\", command=save_sentence, font=(\"Arial\", 14), bg=\"#0F3460\", fg=\"white\", padx=10, pady=5)\n",
    "# save_button.grid(row=0, column=0, padx=10)\n",
    "\n",
    "# remove_button = Button(button_frame, text=\"Remove Last (R)\", command=remove_last_word, font=(\"Arial\", 14), bg=\"#E94560\", fg=\"white\", padx=10, pady=5)\n",
    "# remove_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "# send_button = Button(button_frame, text=\"Send\", command=send_to_client, font=(\"Arial\", 12), bg=\"#3498DB\", fg=\"white\", padx=10, pady=5)\n",
    "# send_button.grid(row=0, column=2, padx=10)\n",
    "\n",
    "# quit_button = Button(button_frame, text=\"Quit (Q)\", command=quit_application, font=(\"Arial\", 14), bg=\"#FF5722\", fg=\"white\", padx=10, pady=5)\n",
    "# quit_button.grid(row=0, column=3, padx=10)  # Changed column index to 3\n",
    "\n",
    "\n",
    "# update_ui()\n",
    "# root.after(1000, process_frame)\n",
    "# import threading\n",
    "\n",
    "# # Run WebSocket server in a separate thread\n",
    "# def start_websocket_server():\n",
    "#     asyncio.run(main())\n",
    "\n",
    "# ws_thread = threading.Thread(target=start_websocket_server, daemon=True)\n",
    "# ws_thread.start()\n",
    "\n",
    "# # Start the Tkinter main loop\n",
    "# root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label, Button, Frame\n",
    "# from PIL import Image, ImageTk\n",
    "# import asyncio\n",
    "# import websockets\n",
    "# import json\n",
    "# import base64\n",
    "# import io\n",
    "# import os\n",
    "# import keyboard\n",
    "# import nest_asyncio\n",
    "# import re\n",
    "# import threading\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Initialize variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# saved_confidence_scores = []\n",
    "# threshold = 0.7\n",
    "# conversation_index = 0\n",
    "# conversation_log = []\n",
    "# normal_sentence = \"Waiting for new messages...\"\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# detection_model = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# MESSAGE_FILE = \"messages.txt\"\n",
    "\n",
    "# def get_next_index():\n",
    "#     if not os.path.exists(MESSAGE_FILE):\n",
    "#         return 1\n",
    "#     with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "#         lines = f.readlines()\n",
    "#         indices = [int(line.split('.')[0]) for line in lines if line.strip() and '.' in line]\n",
    "#         return max(indices) + 1 if indices else 1\n",
    "\n",
    "# def get_last_sign_user_message():\n",
    "#     try:\n",
    "#         with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in reversed(lines):\n",
    "#                 if \"sign_user:\" in line:\n",
    "#                     clean_message = line.replace(\"sign_user:\", \"\").strip()\n",
    "#                     clean_message = re.sub(r'\\d+', '', clean_message)\n",
    "#                     return clean_message.strip()\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"messages.txt not found.\")\n",
    "#     return None\n",
    "\n",
    "# def visualize_skeleton(keypoints_sequence, output_path):\n",
    "#     frame_h, frame_w, fps = 720, 1280, 40\n",
    "#     video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_w, frame_h))\n",
    "\n",
    "#     POSE_CONNECTIONS = [(11,12),(12,14),(14,16),(11,13),(13,15),(11,23),(12,24),\n",
    "#                         (23,24),(23,25),(24,26),(25,27),(27,29),(26,28),(28,30),\n",
    "#                         (23,11),(24,12),(11,12),(23,24)]\n",
    "#     HAND_CONNECTIONS = [(0,1),(1,2),(2,3),(3,4),(0,5),(5,6),(6,7),(7,8),\n",
    "#                         (0,9),(9,10),(10,11),(11,12),(0,13),(13,14),(14,15),(15,16),\n",
    "#                         (0,17),(17,18),(18,19),(19,20)]\n",
    "#     FACE_CONNECTIONS = [(i, i+1) for i in range(467)]\n",
    "\n",
    "#     def draw_connections(frame, kpts, connections, color, thickness=3):\n",
    "#         for p1, p2 in connections:\n",
    "#             if all(0 <= idx < len(kpts) for idx in [p1, p2]):\n",
    "#                 pt1 = (int(kpts[p1][0] * frame_w), int(kpts[p1][1] * frame_h))\n",
    "#                 pt2 = (int(kpts[p2][0] * frame_w), int(kpts[p2][1] * frame_h))\n",
    "#                 cv2.line(frame, pt1, pt2, color, thickness)\n",
    "\n",
    "#     for keypoints in keypoints_sequence:\n",
    "#         frame = np.zeros((frame_h, frame_w, 3), dtype=np.uint8)\n",
    "#         keypoints = keypoints.reshape(-1, 3)\n",
    "\n",
    "#         draw_connections(frame, keypoints[0:468], FACE_CONNECTIONS, (200,200,200), 1)\n",
    "#         draw_connections(frame, keypoints[468:501], POSE_CONNECTIONS, (0, 165, 255), 4)\n",
    "#         draw_connections(frame, keypoints[501:522], HAND_CONNECTIONS, (255, 215, 0), 3)\n",
    "#         draw_connections(frame, keypoints[522:543], HAND_CONNECTIONS, (30, 144, 255), 3)\n",
    "\n",
    "#         for x, y, _ in keypoints:\n",
    "#             if x > 0 and y > 0:\n",
    "#                 cv2.circle(frame, (int(x * frame_w), int(y * frame_h)), 4, (50,205,50), -1)\n",
    "\n",
    "#         video_out.write(frame)\n",
    "\n",
    "#     video_out.release()\n",
    "\n",
    "# def play_video_in_ui(video_path):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#     def show_frame():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             cap.release()\n",
    "#             return\n",
    "\n",
    "#         frame = cv2.resize(frame, (640, 360))\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         img = Image.fromarray(frame)\n",
    "#         imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "#         skeleton_video_label.imgtk = imgtk\n",
    "#         skeleton_video_label.configure(image=imgtk)\n",
    "\n",
    "#         root.after(30, show_frame)\n",
    "\n",
    "#     show_frame()\n",
    "\n",
    "\n",
    "# def update_ui():\n",
    "#     global sentence, normal_sentence\n",
    "#     sentence_label.config(text=f\"Sign: {' '.join(sentence)}\")\n",
    "#     normal_label.config(text=f\"Normal: {normal_sentence}\")\n",
    "#     confidence_text = \"\\n\".join([f\"{word}: {score:.2f}\" for word, score in zip(sentence, saved_confidence_scores)])\n",
    "#     confidence_label.config(text=f\"Confidence:\\n{confidence_text}\")\n",
    "#     root.after(100, update_ui)\n",
    "\n",
    "# def process_frame():\n",
    "#     global sequence, sentence, predictions, saved_confidence_scores, conversation_index, normal_sentence\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         return\n",
    "\n",
    "#     image, results = mediapipe_detection(frame, detection_model)\n",
    "\n",
    "#     if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-30:]\n",
    "\n",
    "#         if len(sequence) == 30:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#             predicted_word = actions[np.argmax(res)]\n",
    "#             predictions.append(np.argmax(res))\n",
    "#             confidence_score = res[np.argmax(res)]\n",
    "\n",
    "#             if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "#                 if confidence_score > threshold:\n",
    "#                     if len(sentence) == 0 or (predicted_word != sentence[-1]):\n",
    "#                         sentence.append(predicted_word)\n",
    "#                         saved_confidence_scores.append(confidence_score)\n",
    "#     else:\n",
    "#         sequence.clear()\n",
    "\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     img = Image.fromarray(frame)\n",
    "#     imgtk = ImageTk.PhotoImage(image=img)\n",
    "#     video_label.imgtk = imgtk\n",
    "#     video_label.configure(image=imgtk)\n",
    "\n",
    "#     root.after(10, process_frame)\n",
    "\n",
    "# def save_sentence():\n",
    "#     global sentence, conversation_index\n",
    "#     if sentence:\n",
    "#         meaningful_response = generate_sentence_gemini(sentence, normal_sentence)\n",
    "#         conversation_log.append(f\"Normal: {normal_sentence}\")\n",
    "#         conversation_log.append(f\"Sign: {meaningful_response}\")\n",
    "#         save_llm_response(meaningful_response)\n",
    "#     sentence.clear()\n",
    "#     saved_confidence_scores.clear()\n",
    "#     conversation_index += 1\n",
    "\n",
    "# def remove_last_word():\n",
    "#     if sentence:\n",
    "#         sentence.pop()\n",
    "#         saved_confidence_scores.pop()\n",
    "\n",
    "# def quit_application():\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     root.quit()\n",
    "#     root.destroy()\n",
    "\n",
    "# connected_clients = set()\n",
    "\n",
    "# async def translation_server(websocket):\n",
    "#     print(\"✅ Production connected.\")\n",
    "#     connected_clients.add(websocket)\n",
    "\n",
    "#     try:\n",
    "#         async for received_data in websocket:\n",
    "#             data = json.loads(received_data)\n",
    "#             question = data.get(\"question\", \"\")\n",
    "#             npy_base64 = data.get(\"npy\", None)\n",
    "\n",
    "#             print(f\"🟢 Received question: {question}\")\n",
    "#             index = get_next_index()\n",
    "\n",
    "#             if npy_base64:\n",
    "#                 npy_bytes = base64.b64decode(npy_base64)\n",
    "#                 npy_array = np.load(io.BytesIO(npy_bytes))\n",
    "#                 np.save(\"received.npy\", npy_array)\n",
    "#                 output_video = \"final_skeleton_output.mp4\"\n",
    "#                 visualize_skeleton(npy_array, output_video)\n",
    "#                 play_video_in_ui(output_video)\n",
    "\n",
    "#             with open(MESSAGE_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(f\"{index}. normal_user: {question}\\n\")\n",
    "\n",
    "#             response = {\"answer\": f\"Processed message #{index}\"}\n",
    "#             await websocket.send(json.dumps(response))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error: {e}\")\n",
    "\n",
    "#     finally:\n",
    "#         connected_clients.remove(websocket)\n",
    "\n",
    "# async def send_to_client():\n",
    "#     sign_user_message = get_last_sign_user_message()\n",
    "#     if sign_user_message:\n",
    "#         response_to_production = {\"sign_user_message\": sign_user_message}\n",
    "#         for client in connected_clients:\n",
    "#             try:\n",
    "#                 await client.send(json.dumps(response_to_production))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"❌ Error sending message: {e}\")\n",
    "\n",
    "# def send_button_clicked():\n",
    "#     print(\"Sending message...\")\n",
    "#     try:\n",
    "#         asyncio.run_coroutine_threadsafe(send_to_client(), event_loop)\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error: {e}\")\n",
    "\n",
    "# async def main():\n",
    "#     async with websockets.serve(translation_server, \"192.168.84.139\", 8779, max_size=50 * 1024 * 1024, ping_interval=None):\n",
    "#         print(\"✅ Server running at ws://192.168.84.139:8779\")\n",
    "#         await asyncio.Future()\n",
    "\n",
    "# # ========== UI DESIGN START ==========\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Sign Language Interpreter\")\n",
    "# root.geometry(\"1000x720\")\n",
    "# root.configure(bg=\"#0F0F3E\")\n",
    "\n",
    "# BG_COLOR = \"#0F0F3E\"\n",
    "# CARD_COLOR = \"#1A1A40\"\n",
    "# BTN_COLOR = \"#00ADB5\"\n",
    "# TEXT_COLOR = \"white\"\n",
    "# FONT_LG = (\"Segoe UI\", 16)\n",
    "# FONT_MD = (\"Segoe UI\", 14)\n",
    "\n",
    "# main_frame = Frame(root, bg=BG_COLOR)\n",
    "# main_frame.pack(fill=\"both\", expand=True, padx=20, pady=20)\n",
    "\n",
    "# left_panel = Frame(main_frame, bg=BG_COLOR)\n",
    "# left_panel.pack(side=\"left\", fill=\"y\", expand=True)\n",
    "\n",
    "# # Create a container for both video cards\n",
    "# video_container = Frame(left_panel, bg=BG_COLOR)\n",
    "# video_container.pack(pady=10, fill=\"both\", expand=True)\n",
    "\n",
    "# # Place original video card in the container, on the left side\n",
    "# video_card = Frame(video_container, bg=CARD_COLOR, bd=0, relief=\"flat\", padx=10, pady=10)\n",
    "# video_card.pack(side=\"left\", fill=\"both\", expand=True, padx=(0, 5))\n",
    "# video_label = Label(video_card, bg=CARD_COLOR)\n",
    "# video_label.pack()\n",
    "\n",
    "# # Place skeleton video card in the container, on the right side\n",
    "# skeleton_video_card = Frame(video_container, bg=CARD_COLOR, bd=0, relief=\"flat\", padx=10, pady=10)\n",
    "# skeleton_video_card.pack(side=\"left\", fill=\"both\", expand=True, padx=(5, 0))\n",
    "# skeleton_video_label = Label(skeleton_video_card, bg=CARD_COLOR)\n",
    "# skeleton_video_label.pack()\n",
    "\n",
    "# # The rest of your code remains the same\n",
    "# sentence_frame = Frame(left_panel, bg=CARD_COLOR, padx=10, pady=10)\n",
    "# sentence_frame.pack(fill=\"x\", pady=10)\n",
    "# sentence_label = Label(sentence_frame, text=\"Sign: \", font=FONT_LG, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\", justify=\"left\")\n",
    "# sentence_label.pack(fill=\"x\")\n",
    "# normal_label = Label(sentence_frame, text=\"Normal: Waiting for new messages...\", font=FONT_LG, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\", justify=\"left\")\n",
    "# confidence_label = Label(sentence_frame, text=\"Confidence:\", font=FONT_MD, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\", justify=\"left\")\n",
    "# confidence_label.pack(fill=\"x\", pady=(10, 0))\n",
    "\n",
    "# right_panel = Frame(main_frame, bg=BG_COLOR)\n",
    "# right_panel.pack(side=\"right\", fill=\"y\", padx=20)\n",
    "# controls_card = Frame(right_panel, bg=CARD_COLOR, padx=15, pady=20)\n",
    "# controls_card.pack(fill=\"both\", expand=True)\n",
    "\n",
    "# Label(controls_card, text=\"Controls\", font=FONT_LG, bg=CARD_COLOR, fg=TEXT_COLOR).pack(pady=(0, 20))\n",
    "\n",
    "# Button(controls_card, text=\"💾 Save (S)\", command=save_sentence, font=FONT_MD, bg=BTN_COLOR, fg=\"white\", relief=\"flat\", padx=10, pady=5).pack(fill=\"x\", pady=10)\n",
    "# Button(controls_card, text=\"⛔ Remove Last (R)\", command=remove_last_word, font=FONT_MD, bg=\"#F55C47\", fg=\"white\", relief=\"flat\", padx=10, pady=5).pack(fill=\"x\", pady=10)\n",
    "# Button(controls_card, text=\"📤 Send\", command=send_button_clicked, font=FONT_MD, bg=\"#3498DB\", fg=\"white\", relief=\"flat\", padx=10, pady=5).pack(fill=\"x\", pady=10)\n",
    "# Button(controls_card, text=\"❌ Quit (Q)\", command=quit_application, font=FONT_MD, bg=\"#D72323\", fg=\"white\", relief=\"flat\", padx=10, pady=5).pack(fill=\"x\", pady=10)\n",
    "\n",
    "# update_ui()\n",
    "# root.after(1000, process_frame)\n",
    "\n",
    "# event_loop = asyncio.new_event_loop()\n",
    "\n",
    "# def start_websocket_server():\n",
    "#     asyncio.set_event_loop(event_loop)\n",
    "#     event_loop.run_until_complete(main())\n",
    "\n",
    "# ws_thread = threading.Thread(target=start_websocket_server, daemon=True)\n",
    "# ws_thread.start()\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Server running at ws://192.168.84.139:8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MADHAV\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "✅ Production connected.\n",
      "🟢 Received question: Where were you when the incident occurred?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Sending message...\n",
      "Sending message...\n",
      "🟢 Received question: Do you suspect anyone?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Sending message...\n",
      "Sending message...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: no close frame received or sent\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tkinter as tk\n",
    "from tkinter import Label, Button, Frame\n",
    "from PIL import Image, ImageTk\n",
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import keyboard\n",
    "import nest_asyncio\n",
    "import re\n",
    "import threading\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "saved_confidence_scores = []\n",
    "threshold = 0.7\n",
    "conversation_index = 0\n",
    "conversation_log = []\n",
    "normal_sentence = \"Waiting for new messages...\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "detection_model = mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "MESSAGE_FILE = \"messages.txt\"\n",
    "\n",
    "def get_next_index():\n",
    "    if not os.path.exists(MESSAGE_FILE):\n",
    "        return 1\n",
    "    with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        indices = [int(line.split('.')[0]) for line in lines if line.strip() and '.' in line]\n",
    "        return max(indices) + 1 if indices else 1\n",
    "\n",
    "def get_last_sign_user_message():\n",
    "    try:\n",
    "        with open(MESSAGE_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in reversed(lines):\n",
    "                if \"sign_user:\" in line:\n",
    "                    clean_message = line.replace(\"sign_user:\", \"\").strip()\n",
    "                    clean_message = re.sub(r'\\d+', '', clean_message)\n",
    "                    return clean_message.strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"messages.txt not found.\")\n",
    "    return None\n",
    "\n",
    "def visualize_skeleton(keypoints_sequence, output_path):\n",
    "    frame_h, frame_w, fps = 720, 1280, 40\n",
    "    video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_w, frame_h))\n",
    "\n",
    "    POSE_CONNECTIONS = [(11,12),(12,14),(14,16),(11,13),(13,15),(11,23),(12,24),\n",
    "                        (23,24),(23,25),(24,26),(25,27),(27,29),(26,28),(28,30),\n",
    "                        (23,11),(24,12),(11,12),(23,24)]\n",
    "    HAND_CONNECTIONS = [(0,1),(1,2),(2,3),(3,4),(0,5),(5,6),(6,7),(7,8),\n",
    "                        (0,9),(9,10),(10,11),(11,12),(0,13),(13,14),(14,15),(15,16),\n",
    "                        (0,17),(17,18),(18,19),(19,20)]\n",
    "    FACE_CONNECTIONS = [(i, i+1) for i in range(467)]\n",
    "\n",
    "    def draw_connections(frame, kpts, connections, color, thickness=3):\n",
    "        for p1, p2 in connections:\n",
    "            if all(0 <= idx < len(kpts) for idx in [p1, p2]):\n",
    "                pt1 = (int(kpts[p1][0] * frame_w), int(kpts[p1][1] * frame_h))\n",
    "                pt2 = (int(kpts[p2][0] * frame_w), int(kpts[p2][1] * frame_h))\n",
    "                cv2.line(frame, pt1, pt2, color, thickness)\n",
    "\n",
    "    for keypoints in keypoints_sequence:\n",
    "        frame = np.zeros((frame_h, frame_w, 3), dtype=np.uint8)\n",
    "        keypoints = keypoints.reshape(-1, 3)\n",
    "\n",
    "        draw_connections(frame, keypoints[0:468], FACE_CONNECTIONS, (200,200,200), 1)\n",
    "        draw_connections(frame, keypoints[468:501], POSE_CONNECTIONS, (0, 165, 255), 4)\n",
    "        draw_connections(frame, keypoints[501:522], HAND_CONNECTIONS, (255, 215, 0), 3)\n",
    "        draw_connections(frame, keypoints[522:543], HAND_CONNECTIONS, (30, 144, 255), 3)\n",
    "\n",
    "        for x, y, _ in keypoints:\n",
    "            if x > 0 and y > 0:\n",
    "                cv2.circle(frame, (int(x * frame_w), int(y * frame_h)), 4, (50,205,50), -1)\n",
    "\n",
    "        video_out.write(frame)\n",
    "\n",
    "    video_out.release()\n",
    "\n",
    "def play_video_in_ui(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    def show_frame():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            return\n",
    "\n",
    "        frame = cv2.resize(frame, (640, 360))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        skeleton_video_label.imgtk = imgtk\n",
    "        skeleton_video_label.configure(image=imgtk)\n",
    "\n",
    "        root.after(30, show_frame)\n",
    "\n",
    "    show_frame()\n",
    "\n",
    "def update_ui():\n",
    "    global sentence, normal_sentence\n",
    "    sentence_label.config(text=f\"Sign: {' '.join(sentence)}\")\n",
    "    normal_label.config(text=f\"Normal: {normal_sentence}\")\n",
    "    confidence_text = \"\\n\".join([f\"{word}: {score:.2f}\" for word, score in zip(sentence, saved_confidence_scores)])\n",
    "    confidence_label.config(text=f\"Confidence:\\n{confidence_text}\")\n",
    "    root.after(100, update_ui)\n",
    "\n",
    "def process_frame():\n",
    "    global sequence, sentence, predictions, saved_confidence_scores, conversation_index, normal_sentence\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    image, results = mediapipe_detection(frame, detection_model)\n",
    "\n",
    "    if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predicted_word = actions[np.argmax(res)]\n",
    "            predictions.append(np.argmax(res))\n",
    "            confidence_score = res[np.argmax(res)]\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if confidence_score > threshold:\n",
    "                    if len(sentence) == 0 or (predicted_word != sentence[-1]):\n",
    "                        sentence.append(predicted_word)\n",
    "                        saved_confidence_scores.append(confidence_score)\n",
    "    else:\n",
    "        sequence.clear()\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    root.after(10, process_frame)\n",
    "\n",
    "def save_sentence():\n",
    "    global sentence, conversation_index\n",
    "    if sentence:\n",
    "        meaningful_response = generate_sentence_gemini(sentence, normal_sentence)\n",
    "        conversation_log.append(f\"Normal: {normal_sentence}\")\n",
    "        conversation_log.append(f\"Sign: {meaningful_response}\")\n",
    "        save_llm_response(meaningful_response)\n",
    "    sentence.clear()\n",
    "    saved_confidence_scores.clear()\n",
    "    conversation_index += 1\n",
    "\n",
    "def remove_last_word():\n",
    "    if sentence:\n",
    "        sentence.pop()\n",
    "        saved_confidence_scores.pop()\n",
    "\n",
    "def quit_application():\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    root.quit()\n",
    "    root.destroy()\n",
    "\n",
    "connected_clients = set()\n",
    "\n",
    "async def translation_server(websocket):\n",
    "    print(\"✅ Production connected.\")\n",
    "    connected_clients.add(websocket)\n",
    "\n",
    "    try:\n",
    "        async for received_data in websocket:\n",
    "            data = json.loads(received_data)\n",
    "            question = data.get(\"question\", \"\")\n",
    "            npy_base64 = data.get(\"npy\", None)\n",
    "\n",
    "            print(f\"🟢 Received question: {question}\")\n",
    "            index = get_next_index()\n",
    "\n",
    "            if npy_base64:\n",
    "                npy_bytes = base64.b64decode(npy_base64)\n",
    "                npy_array = np.load(io.BytesIO(npy_bytes))\n",
    "                np.save(\"received.npy\", npy_array)\n",
    "                output_video = \"final_skeleton_output.mp4\"\n",
    "                visualize_skeleton(npy_array, output_video)\n",
    "                play_video_in_ui(output_video)\n",
    "\n",
    "            with open(MESSAGE_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"{index}. normal_user: {question}\\n\")\n",
    "\n",
    "            response = {\"answer\": f\"Processed message #{index}\"}\n",
    "            await websocket.send(json.dumps(response))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        connected_clients.remove(websocket)\n",
    "\n",
    "async def send_to_client():\n",
    "    sign_user_message = get_last_sign_user_message()\n",
    "    if sign_user_message:\n",
    "        response_to_production = {\"sign_user_message\": sign_user_message}\n",
    "        for client in connected_clients:\n",
    "            try:\n",
    "                await client.send(json.dumps(response_to_production))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error sending message: {e}\")\n",
    "\n",
    "def send_button_clicked():\n",
    "    print(\"Sending message...\")\n",
    "    try:\n",
    "        asyncio.run_coroutine_threadsafe(send_to_client(), event_loop)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "async def main():\n",
    "    async with websockets.serve(translation_server, \"192.168.84.139\", 8779, max_size=50 * 1024 * 1024, ping_interval=None):\n",
    "        print(\"✅ Server running at ws://192.168.84.139:8779\")\n",
    "        await asyncio.Future()\n",
    "\n",
    "# ========== UI DESIGN START ==========\n",
    "root = tk.Tk()\n",
    "root.title(\"Sign Language Interpreter\")\n",
    "root.geometry(\"1100x720\")\n",
    "root.configure(bg=\"#1F1D36\")\n",
    "\n",
    "BG_COLOR = \"#292328\"\n",
    "CARD_COLOR = \"#3F3351\"\n",
    "SECONDARY_CARD = \"#3F3351\"\n",
    "BTN_COLOR = \"#00ADB5\"\n",
    "DANGER_COLOR = \"#FF4C4C\"\n",
    "TEXT_COLOR = \"#EEEEEE\"\n",
    "FONT_LG = (\"Segoe UI Semibold\", 16)\n",
    "FONT_MD = (\"Segoe UI\", 14)\n",
    "\n",
    "main_frame = Frame(root, bg=BG_COLOR)\n",
    "main_frame.pack(fill=\"both\", expand=True, padx=20, pady=20)\n",
    "\n",
    "left_panel = Frame(main_frame, bg=BG_COLOR)\n",
    "left_panel.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "video_container = Frame(left_panel, bg=BG_COLOR)\n",
    "video_container.pack(pady=10, fill=\"both\", expand=True)\n",
    "\n",
    "video_card = Frame(video_container, bg=CARD_COLOR, bd=2, relief=\"groove\", padx=10, pady=10)\n",
    "video_card.pack(side=\"left\", fill=\"both\", expand=True, padx=(0, 10))\n",
    "Label(video_card, text=\"Webcam Feed\", font=FONT_MD, bg=CARD_COLOR, fg=TEXT_COLOR).pack(anchor=\"w\")\n",
    "\n",
    "video_label = Label(video_card, bg=CARD_COLOR)\n",
    "video_label.pack(expand=True)\n",
    "\n",
    "skeleton_video_card = Frame(video_container, bg=SECONDARY_CARD, bd=2, relief=\"groove\", padx=10, pady=10)\n",
    "skeleton_video_card.pack(side=\"left\", fill=\"both\", expand=True, padx=(10, 0))\n",
    "Label(skeleton_video_card, text=\"Other Person\", font=FONT_MD, bg=SECONDARY_CARD, fg=TEXT_COLOR).pack(anchor=\"w\")\n",
    "skeleton_video_label = Label(skeleton_video_card, bg=SECONDARY_CARD)\n",
    "skeleton_video_label.pack(expand=True)\n",
    "\n",
    "sentence_frame = Frame(left_panel, bg=CARD_COLOR, padx=10, pady=10)\n",
    "sentence_frame.pack(fill=\"x\", pady=15)\n",
    "sentence_label = Label(sentence_frame, text=\"📝 Sign:\", font=FONT_LG, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\")\n",
    "sentence_label.pack(fill=\"x\")\n",
    "normal_label = Label(sentence_frame, text=\"Normal: Waiting for new messages...\", font=FONT_MD, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\", justify=\"left\")\n",
    "confidence_label = Label(sentence_frame, text=\"📊 Confidence:\", font=FONT_MD, bg=CARD_COLOR, fg=TEXT_COLOR, anchor=\"w\")\n",
    "confidence_label.pack(fill=\"x\", pady=(10, 0))\n",
    "\n",
    "right_panel = Frame(main_frame, bg=BG_COLOR)\n",
    "right_panel.pack(side=\"right\", fill=\"y\", padx=20)\n",
    "controls_card = Frame(right_panel, bg=CARD_COLOR, padx=20, pady=20)\n",
    "controls_card.pack(fill=\"both\", expand=True)\n",
    "\n",
    "Label(controls_card, text=\"🧭 Controls\", font=FONT_LG, bg=CARD_COLOR, fg=TEXT_COLOR).pack(pady=(0, 20))\n",
    "\n",
    "def create_btn(text, cmd, bg=BTN_COLOR, fg=\"white\"):\n",
    "    btn = Button(controls_card, text=text, command=cmd, font=FONT_MD, bg=bg, fg=fg, relief=\"flat\", padx=10, pady=8)\n",
    "    btn.pack(fill=\"x\", pady=8)\n",
    "    return btn\n",
    "\n",
    "create_btn(\"💾 Save (S)\", save_sentence)\n",
    "create_btn(\"⛔ Remove Last (R)\", remove_last_word, bg=\"#F55C47\")\n",
    "create_btn(\"📤 Send\", send_button_clicked, bg=\"#3498DB\")\n",
    "create_btn(\"❌ Quit (Q)\", quit_application, bg=DANGER_COLOR)\n",
    "\n",
    "update_ui()\n",
    "root.after(1000, process_frame)\n",
    "\n",
    "event_loop = asyncio.new_event_loop()\n",
    "\n",
    "def start_websocket_server():\n",
    "    asyncio.set_event_loop(event_loop)\n",
    "    event_loop.run_until_complete(main())\n",
    "\n",
    "ws_thread = threading.Thread(target=start_websocket_server, daemon=True)\n",
    "ws_thread.start()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
